# ============================================
# HARBOR WORKER CONFIGURATION EXAMPLE
# ============================================
# This is a template for worker EC2 instances.
# Copy this file to .env and update with your values.
#
# Workers are part of a distributed pool that processes
# Harbor tasks from the Redis queue.

# ============================================
# DATABASE (PostgreSQL)
# ============================================
# Point to your shared PostgreSQL instance
# Replace POSTGRES_IP with actual IP (e.g., 10.0.1.50)
DATABASE_URL=postgresql+asyncpg://postgres:postgres@POSTGRES_IP:5432/tbench

# ============================================
# REDIS (Task Queue)
# ============================================
# Point to your shared Redis instance
# Replace REDIS_IP with actual IP (e.g., 10.0.1.51)
REDIS_URL=redis://REDIS_IP:6379/0

# ============================================
# OPENROUTER API
# ============================================
# Default API key for LLM models
# Can be overridden per job by users
DEFAULT_OPENROUTER_KEY=sk-or-v1-your-key-here

# ============================================
# SHARED STORAGE (NFS)
# ============================================
# Path to NFS-mounted shared storage
# All workers MUST use the same path
# Make sure NFS is mounted before starting worker!
JOBS_DIR=/shared/harbor-jobs/jobs
UPLOADS_DIR=/shared/harbor-jobs/uploads

# ============================================
# HARBOR SETTINGS
# ============================================
# Timeout multiplier for Harbor tasks (1.0 = default timeout)
HARBOR_TIMEOUT_MULTIPLIER=1.0

# ============================================
# DOCKER CONFIGURATION
# ============================================
# Leave empty to use local Docker socket (recommended)
# Workers run Harbor + Docker locally
DOCKER_HOST=
DOCKER_TLS_VERIFY=0
DOCKER_CERT_PATH=

# ============================================
# CORS (Not used by workers, but required by config)
# ============================================
CORS_ORIGINS=["http://localhost:3000"]

# ============================================
# NOTES
# ============================================
# Worker concurrency is controlled by Celery startup command,
# not by environment variables. See start_worker.sh or systemd service.
#
# Typical concurrency values:
#   - t3.2xlarge (8 vCPU, 32GB RAM): 60 concurrent jobs
#   - t3.xlarge (4 vCPU, 16GB RAM): 30 concurrent jobs
#   - t3.large (2 vCPU, 8GB RAM): 15 concurrent jobs
